{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"results.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset (3,4): 4 4 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game size\n",
    "Larger game sizes are better: 10 or 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for game size 3: 0.9142084374819717\n",
      "Mean accuracy for game size 10: 0.9616841533117824\n",
      "Mean accuracy for game size 20: 0.968630887567997\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[data['game_size'] == 3, 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for game size 3:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[data['game_size'] == 10, 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for game size 10:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[data['game_size'] == 20, 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for game size 20:\", mean_train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size\n",
    "Batch size does not really seem to matter, but smaller is a bit better: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for batch size 32: 0.9498066183638899\n",
      "Mean accuracy for batch size 64: 0.9485835441284709\n",
      "Mean accuracy for batch size 128: 0.9456388966904746\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[data['batch_size'] == 32, 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for batch size 32:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[data['batch_size'] == 64, 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for batch size 64:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[data['batch_size'] == 128, 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for batch size 128:\", mean_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for batch size 32 and game size 10 or 20: 0.9689533325533072\n",
      "Mean accuracy for batch size 64: 0.9656351047257582\n",
      "Mean accuracy for batch size 128: 0.9608841240406036\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[(data['batch_size'] == 32) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for batch size 32 and game size 10 or 20:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['batch_size'] == 64) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for batch size 64:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['batch_size'] == 128) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for batch size 128:\", mean_train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "Does not matter a lot but smaller is better: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for learning rate 0.0005 and game size 10 or 20: 0.9677117218573889\n",
      "Mean accuracy for learning rate 0.001: 0.9626033190223906\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[(data['learning_rate'] == 0.0005) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for learning rate 0.0005 and game size 10 or 20:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['learning_rate'] == 0.001) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for learning rate 0.001:\", mean_train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden size\n",
    "Does not really matter but smaller is better: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for hidden size 128 and game size 10 or 20: 0.9658779733710818\n",
      "Mean accuracy for hidden size 256: 0.9644370675086975\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[(data['hidden_size'] == 128) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for hidden size 128 and game size 10 or 20:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['hidden_size'] == 256) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for hidden size 256:\", mean_train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature\n",
    "Higher is better: 1.5 or 2 rather than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for temperature 1 and game size 10 or 20: 0.9398398324847221\n",
      "Mean accuracy for temperature 1.5: 0.9743879847228527\n",
      "Mean accuracy for temperature 2: 0.9812447441120943\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[(data['temperature'] == 1.0) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for temperature 1 and game size 10 or 20:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['temperature'] == 1.5) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for temperature 1.5:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['temperature'] == 2.0) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for temperature 2:\", mean_train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature update\n",
    "Closer to 1 is better: 0.99 rather than 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for temperature update 0.97 and game size 10 or 20: 0.9560216731495328\n",
      "Mean accuracy for temperature update 0.99: 0.9742933677302467\n"
     ]
    }
   ],
   "source": [
    "mean_train_acc = data.loc[(data['temp_update'] == 0.97) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for temperature update 0.97 and game size 10 or 20:\", mean_train_acc)\n",
    "mean_train_acc = data.loc[(data['temp_update'] == 0.99) & (data['game_size'] > 3), 'train_accuracy'].mean()\n",
    "print(\"Mean accuracy for temperature update 0.99:\", mean_train_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
