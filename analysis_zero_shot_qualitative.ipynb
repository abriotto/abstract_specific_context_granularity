{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T09:28:25.699600Z",
     "start_time": "2024-08-29T09:28:25.680342Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.load_results import *\n",
    "from utils.plot_helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "import torch\n",
    "from utils.analysis_from_interaction import *\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "import os\n",
    "if not os.path.exists('analysis'):\n",
    "    os.makedirs('analysis')\n",
    "#import plotly.express as px\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def objects_to_concepts(sender_input):\n",
    "    \"\"\"reconstruct concepts from objects in interaction\"\"\"\n",
    "    n_targets = int(sender_input.shape[1]/2)\n",
    "    # get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    return concepts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def retrieve_messages(interaction):\n",
    "    \"\"\"retrieve messages from interaction\"\"\"\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    return messages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def count_symbols(messages):\n",
    "    \"\"\"counts symbols in messages\"\"\"\n",
    "    all_symbols = [symbol for message in messages for symbol in message]\n",
    "    symbol_counts = Counter(all_symbols)\n",
    "    return symbol_counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def get_unique_message_set(messages):\n",
    "    \"\"\"returns unique messages as a set ready for set operations\"\"\"\n",
    "    return set(tuple(message) for message in messages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def get_unique_concept_set(concepts):\n",
    "    \"\"\"returns unique concepts\"\"\"\n",
    "    concept_tuples = []\n",
    "    for objects, fixed in concepts:\n",
    "        tuple_objects = []\n",
    "        for object in objects:\n",
    "            tuple_objects.append(tuple(object))\n",
    "        tuple_objects = tuple(tuple_objects)\n",
    "        tuple_concept = (tuple_objects, tuple(fixed))\n",
    "        concept_tuples.append(tuple_concept)\n",
    "    tuple(concept_tuples)\n",
    "    unique_concepts = set(concept_tuples)\n",
    "    return unique_concepts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T12:32:55.524661Z",
     "start_time": "2024-08-29T12:32:55.498126Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)']\n",
    "n_values = [4, 8, 16, 4, 8, 4]\n",
    "n_attributes = [3, 3, 3, 4, 4, 5]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T13:21:16.656454Z",
     "start_time": "2024-08-29T13:21:16.641185Z"
    }
   },
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "zero_shot = True # whether zero-shot simulations are evaluated\n",
    "zero_shot_test = 'generic' # 'generic' or 'specific'\n",
    "test_interactions = True # whether scores should be calculated on test interactions (only with zero shot)\n",
    "setting = \"\"\n",
    "if context_unaware:\n",
    "    setting = setting + 'context_unaware'\n",
    "else:\n",
    "    setting = setting + 'standard'\n",
    "if zero_shot:\n",
    "    setting = setting + '/zero_shot/' + zero_shot_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine vocab size and message reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T13:22:46.636959Z",
     "start_time": "2024-08-29T13:21:18.523353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,4)\n",
      "14 messages used for the 120 novel concepts\n",
      "14 messages used for the 120 novel concepts\n",
      "25 messages used for the 120 novel concepts\n",
      "5 messages used for the 120 novel concepts\n",
      "11 messages used for the 120 novel concepts\n",
      "(3,8)\n",
      "25 messages used for the 240 novel concepts\n",
      "28 messages used for the 240 novel concepts\n",
      "32 messages used for the 240 novel concepts\n",
      "19 messages used for the 240 novel concepts\n",
      "49 messages used for the 240 novel concepts\n",
      "(3,16)\n",
      "82 messages used for the 480 novel concepts\n",
      "63 messages used for the 480 novel concepts\n",
      "56 messages used for the 480 novel concepts\n",
      "43 messages used for the 480 novel concepts\n",
      "60 messages used for the 480 novel concepts\n",
      "(4,4)\n",
      "7 messages used for the 160 novel concepts\n",
      "33 messages used for the 160 novel concepts\n",
      "21 messages used for the 160 novel concepts\n",
      "15 messages used for the 160 novel concepts\n",
      "21 messages used for the 160 novel concepts\n",
      "(4,8)\n",
      "5 messages used for the 320 novel concepts\n",
      "22 messages used for the 320 novel concepts\n",
      "10 messages used for the 320 novel concepts\n",
      "0 messages used for the 320 novel concepts\n",
      "25 messages used for the 320 novel concepts\n",
      "(5,4)\n",
      "8 messages used for the 200 novel concepts\n",
      "5 messages used for the 200 novel concepts\n",
      "0 messages used for the 200 novel concepts\n",
      "0 messages used for the 200 novel concepts\n",
      "0 messages used for the 200 novel concepts\n"
     ]
    }
   ],
   "source": [
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    print(d)\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_test = (path_to_run + 'interactions/test/epoch_0/interaction_gpu0')\n",
    "        interaction_train = torch.load(path_to_interaction_train)\n",
    "        interaction_val = torch.load(path_to_interaction_val)\n",
    "        interaction_test = torch.load(path_to_interaction_test)\n",
    "        \n",
    "        concepts_train = objects_to_concepts(interaction_train.sender_input)\n",
    "        concepts_val = objects_to_concepts(interaction_val.sender_input)\n",
    "        concepts_test = objects_to_concepts(interaction_test.sender_input)\n",
    "        \n",
    "        messages_train = retrieve_messages(interaction_train)\n",
    "        messages_val = retrieve_messages(interaction_val)\n",
    "        messages_test = retrieve_messages(interaction_test)\n",
    "    \n",
    "        symbol_counts_train = count_symbols(messages_train)\n",
    "        symbol_counts_val = count_symbols(messages_val)\n",
    "        symbol_counts_test = count_symbols(messages_test)\n",
    "        symbol_counts = [symbol_counts_train, symbol_counts_val, symbol_counts_test]\n",
    "        pickle.dump(symbol_counts, open(path_to_run + 'symbol_counts.pkl', 'wb'))\n",
    "        \n",
    "        # consider train and validation messages together\n",
    "        messages_train_val = messages_train +  messages_val\n",
    "        # consider only unique messages\n",
    "        messages_train_val_unique = get_unique_message_set(messages_train_val)\n",
    "        messages_test_unique = get_unique_message_set(messages_test)\n",
    "        # total messages\n",
    "        messages_total = messages_train_val +  messages_test\n",
    "        messages_total_unique = get_unique_message_set(messages_total)\n",
    "        \n",
    "        # concepts\n",
    "        concepts_train_unique = get_unique_concept_set(concepts_train)\n",
    "        concepts_val_unique = get_unique_concept_set(concepts_val)\n",
    "        concepts_test_unique = get_unique_concept_set(concepts_test)\n",
    "        concepts_total = concepts_train + concepts_val + concepts_test\n",
    "        concepts_total_unique = get_unique_concept_set(concepts_total)\n",
    "        num_of_concepts = [len(concepts_train_unique), len(concepts_val_unique), len(concepts_test_unique), len(concepts_total_unique)]\n",
    "        pickle.dump(num_of_concepts, open(path_to_run + 'num_of_concepts.pkl', 'wb'))\n",
    "        \n",
    "        # messages reused in testing:\n",
    "        intersection = messages_train_val_unique & messages_test_unique\n",
    "        \n",
    "        # messages only used in training:\n",
    "        difference_train = messages_train_val_unique - messages_test_unique\n",
    "        \n",
    "        # messages only used in testing:\n",
    "        difference_test = messages_test_unique - messages_train_val_unique\n",
    "        print(len(difference_test), \"messages used for the\", len(concepts_test_unique), \"novel concepts\")\n",
    "        \n",
    "        message_reuse = [len(intersection), len(difference_train), len(difference_test), len(concepts_test_unique), (len(difference_test)/len(concepts_test_unique))]\n",
    "        pickle.dump(message_reuse, open(path_to_run + 'message_reuse.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "message_reuse_dict = {'intersection': [], 'difference train': [], 'difference test': [], 'concepts test unique': [], 'test ratio': []}\n",
    "for i, d in enumerate(datasets):\n",
    "    intersection, train_difference, test_difference, test_concepts, test_ratio = [], [], [], [], []\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        message_reuse = pickle.load(open(path_to_run + 'message_reuse.pkl', 'rb'))\n",
    "        intersection.append(message_reuse[0])\n",
    "        train_difference.append(message_reuse[1])\n",
    "        test_difference.append(message_reuse[2])\n",
    "        test_concepts.append(message_reuse[3])\n",
    "        test_ratio.append(message_reuse[4])\n",
    "    message_reuse_dict['intersection'].append(intersection)\n",
    "    message_reuse_dict['difference train'].append(train_difference)\n",
    "    message_reuse_dict['difference test'].append(test_difference)\n",
    "    message_reuse_dict['concepts test unique'].append(test_concepts)\n",
    "    message_reuse_dict['test ratio'].append(test_ratio)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T13:22:46.648280Z",
     "start_time": "2024-08-29T13:22:46.644131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "{} &    novel messages & novel\\n concepts &            ratio \\\\\n",
      "\\midrule\n",
      "D(3,4)  &   13.8 $\\pm$ 6.49 &              120 &  0.11 $\\pm$ 0.05 \\\\\n",
      "D(3,8)  &  30.6 $\\pm$ 10.13 &              240 &  0.13 $\\pm$ 0.04 \\\\\n",
      "D(3,16) &  60.8 $\\pm$ 12.61 &              480 &  0.13 $\\pm$ 0.03 \\\\\n",
      "D(4,4)  &   19.4 $\\pm$ 8.52 &              160 &  0.12 $\\pm$ 0.05 \\\\\n",
      "D(4,8)  &   12.4 $\\pm$ 9.65 &              320 &  0.04 $\\pm$ 0.03 \\\\\n",
      "D(5,4)  &    2.6 $\\pm$ 3.32 &              200 &  0.01 $\\pm$ 0.02 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/03rlh8jd6nqbws0_xg9jh20w0000gq/T/ipykernel_25511/387303971.py:35: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table = df.to_latex(index=True, escape=False)\n"
     ]
    }
   ],
   "source": [
    "message_reuse = [message_reuse_dict['difference test'], message_reuse_dict['concepts test unique'], message_reuse_dict['test ratio']]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "mess_reuse_array = np.array(message_reuse)\n",
    "\n",
    "# Compute means and standard deviations over the five runs\n",
    "means = np.mean(mess_reuse_array, axis=-1)\n",
    "std_devs = np.std(mess_reuse_array, axis=-1)\n",
    "\n",
    "# Row names and column names\n",
    "row_names = [\"D(3,4)\", \"D(3,8)\", \"D(3,16)\", \"D(4,4)\", \"D(4,8)\", \"D(5,4)\"]\n",
    "col_names = [\"novel messages\", \"novel concepts\", \"ratio\"]\n",
    "\n",
    "# Prepare the data for the DataFrames\n",
    "data = []\n",
    "\n",
    "# iterate over datasets\n",
    "for i in range(means.shape[1]):\n",
    "    row = []\n",
    "    # iterate over conditions\n",
    "    for j in range(means.shape[0]):\n",
    "        if j == 0:\n",
    "            formatted_value = f\"{means[j, i]:.1f} $\\\\pm$ {std_devs[j, i]:.2f}\"\n",
    "        elif j == 1:\n",
    "            formatted_value = f\"{int(means[j, i])}\"\n",
    "        else:\n",
    "            formatted_value = f\"{means[j, i]:.2f} $\\\\pm$ {std_devs[j, i]:.2f}\"\n",
    "        row.append(formatted_value)\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrames\n",
    "df = pd.DataFrame(data, index=row_names, columns=col_names)\n",
    "\n",
    "# Convert DataFrames to LaTeX tables\n",
    "latex_table = df.to_latex(index=True, escape=False)\n",
    "\n",
    "print(latex_table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T13:25:32.493507Z",
     "start_time": "2024-08-29T13:25:32.465315Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
