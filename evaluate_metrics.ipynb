{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils.analysis_from_interaction import *\n",
    "from egg.core.language_analysis import Disent\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate metrics from stored interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = False # whether original or control simulations are evaluated\n",
    "\n",
    "if control:\n",
    "    datasets = ('(4,8)', '(4,8)', '(4,8)', '(4,8)', '(4,8)', '(4,8)', '(4,8)')\n",
    "    n_attributes = (4, 4, 4, 4, 4, 4, 4)\n",
    "    n_values = (8, 8, 8, 8, 8, 8, 8)\n",
    "    n_epochs = 300\n",
    "    paths = ['results/(4,8)_sample_scaling_10_balanced_True_vsf_1/', \n",
    "             'results/(4,8)_sample_scaling_10_balanced_False_vsf_1/',\n",
    "             'results/(4,8)_sample_scaling_10_balanced_True_vsf_2/', \n",
    "             'results/(4,8)_sample_scaling_10_balanced_False_vsf_2/',\n",
    "             'results/(4,8)_sample_scaling_10_balanced_True_vsf_3/', \n",
    "             'results/(4,8)_sample_scaling_10_balanced_True_vsf_4/', \n",
    "             'results/(4,8)_sample_scaling_10_balanced_False_vsf_4/',]\n",
    "    \n",
    "else: \n",
    "    datasets = ('(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)')\n",
    "    n_attributes = (3, 3, 3, 4, 4, 5)\n",
    "    n_values = (4, 8, 16, 4, 8, 4)\n",
    "    #n_epochs = 300\n",
    "    n_epochs = 100\n",
    "    #paths = ['results/' + d + '_sample_scaling_10_balanced_False_vsf_3/' for d in datasets]\n",
    "    paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_unaware = True # whether original or context_unaware simulations are evaluated\n",
    "if context_unaware:\n",
    "    setting = 'context_unaware'\n",
    "else:\n",
    "    setting = 'standard'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy scores: MI, effectiveness, efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' # do same for 'context_unaware' instead of 'standard'\n",
    "        #path_to_interaction = (path_to_run + 'interactions/train/interactions_epoch' + str(n_epochs))\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = information_scores(interaction, attributes, values, normalizer=\"arithmetic\")\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'entropy_scores.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we evaluated message length per hierarchy level after training but \n",
    "# you can also use the HierarchicalMessageLength callback and store the results \n",
    "# TODO: Message length results look weird, needs to be fixed!\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        #path_to_interaction = (path_to_run + 'interactions/train/interactions_epoch' + str(n_epochs))\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = message_length_per_hierarchy_level(interaction, attributes)\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'message_length_hierarchical.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  symbol redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet implemented:\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    attributes = n_attributes[d]\n",
    "    values = n_values[d]\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        symbol_f = np.load(path_to_run + 'symbols_pernsame.npy')\n",
    "        #path_to_interaction = (path_to_run + 'interactions/train/interactions_epoch' + str(n_epochs))\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "        redundancy, MI = symbol_frequency(interaction, attributes, values, vocab_size)\n",
    "        \n",
    "        scores = {'symbol_redundancy': redundancy, 'MI_symbol-attribute_value': MI}\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'symbol_redundancy.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  compositionality scores: topsim, posdis, bosdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset (3,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4239018353653433, 'topsim_specific_train': 0.5103873491798231, 'topsim_test': 0.4371581732605145, 'topsim_specific_test': 0.5133992917589033}\n",
      "dataset (3,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3934997453936548, 'topsim_specific_train': 0.4528229888327764, 'topsim_test': 0.4029663547474446, 'topsim_specific_test': 0.4598832100611965}\n",
      "dataset (3,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4684454171670405, 'topsim_specific_train': 0.5228703913145929, 'topsim_test': 0.45989264302761873, 'topsim_specific_test': 0.516067972430117}\n",
      "dataset (3,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4379705660962943, 'topsim_specific_train': 0.469714858941141, 'topsim_test': 0.4320143945098909, 'topsim_specific_test': 0.48134552598615715}\n",
      "dataset (3,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4361416470989032, 'topsim_specific_train': 0.49259186887998935, 'topsim_test': 0.4265305128485554, 'topsim_specific_test': 0.4894776196341643}\n",
      "dataset (3,8) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3137100901325268, 'topsim_specific_train': 0.3677652836855773, 'topsim_test': 0.31626188787807036, 'topsim_specific_test': 0.3639222395171343}\n",
      "dataset (3,8) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.32840941901133974, 'topsim_specific_train': 0.38897881926143635, 'topsim_test': 0.3361413921758745, 'topsim_specific_test': 0.3977022484335317}\n",
      "dataset (3,8) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.353794320403824, 'topsim_specific_train': 0.4147389304529133, 'topsim_test': 0.3489598363311656, 'topsim_specific_test': 0.41528991544580285}\n",
      "dataset (3,8) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3819071013480362, 'topsim_specific_train': 0.43905876788297965, 'topsim_test': 0.38569741541870356, 'topsim_specific_test': 0.43663678664615874}\n",
      "dataset (3,8) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.316840397286699, 'topsim_specific_train': 0.3640470457732016, 'topsim_test': 0.3083297700502934, 'topsim_specific_test': 0.35548492476543503}\n",
      "dataset (3,16) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4448934709667058, 'topsim_specific_train': 0.5238511216778622, 'topsim_test': 0.4360564376561684, 'topsim_specific_test': 0.5125676264721706}\n",
      "dataset (3,16) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.19654257447797904, 'topsim_specific_train': 0.22770373288994683, 'topsim_test': 0.19538788185591152, 'topsim_specific_test': 0.22621279801475602}\n",
      "dataset (3,16) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.2454068931301526, 'topsim_specific_train': 0.2907945701868291, 'topsim_test': 0.2430953975487209, 'topsim_specific_test': 0.2865157348472092}\n",
      "dataset (3,16) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.2279888207861302, 'topsim_specific_train': 0.26860972820110357, 'topsim_test': 0.22585296284458822, 'topsim_specific_test': 0.267460322544631}\n",
      "dataset (3,16) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.2400420170636324, 'topsim_specific_train': 0.2782120214399253, 'topsim_test': 0.2319973906908479, 'topsim_specific_test': 0.2739335967270399}\n",
      "dataset (4,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.5372081612544628, 'topsim_specific_train': 0.5837618887335098, 'topsim_test': 0.5381586956553076, 'topsim_specific_test': 0.597415372704856}\n",
      "dataset (4,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.5361583665381506, 'topsim_specific_train': 0.6190171880789712, 'topsim_test': 0.5444067884666872, 'topsim_specific_test': 0.6267585669256982}\n",
      "dataset (4,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.5609119593079029, 'topsim_specific_train': 0.6488447463084999, 'topsim_test': 0.5569764715856824, 'topsim_specific_test': 0.6508552049857604}\n",
      "dataset (4,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.49693293099130076, 'topsim_specific_train': 0.5599849229021092, 'topsim_test': 0.5097793498176034, 'topsim_specific_test': 0.5757749567173266}\n",
      "dataset (4,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.5623296476586429, 'topsim_specific_train': 0.6726272067034023, 'topsim_test': 0.5940406311218391, 'topsim_specific_test': 0.6978359895551542}\n",
      "dataset (4,8) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.36279911015871075, 'topsim_specific_train': 0.4252031771107589, 'topsim_test': 0.3573659450495993, 'topsim_specific_test': 0.4249423681930525}\n",
      "dataset (4,8) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3493183857551906, 'topsim_specific_train': 0.39407405010843205, 'topsim_test': 0.33633487157858905, 'topsim_specific_test': 0.38738539550951034}\n",
      "dataset (4,8) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3981646368199721, 'topsim_specific_train': 0.4561128027223481, 'topsim_test': 0.398707362148107, 'topsim_specific_test': 0.4699631220034621}\n",
      "dataset (4,8) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4258171001131326, 'topsim_specific_train': 0.49055874335879823, 'topsim_test': 0.4233151527396001, 'topsim_specific_test': 0.49768389931680257}\n",
      "dataset (4,8) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.31141182614422763, 'topsim_specific_train': 0.3632471769876746, 'topsim_test': 0.3122743643973317, 'topsim_specific_test': 0.36279132247787815}\n",
      "dataset (5,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4795961894677005, 'topsim_specific_train': 0.5244343171027428, 'topsim_test': 0.479797783618233, 'topsim_specific_test': 0.5313269985641602}\n",
      "dataset (5,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4826134336806546, 'topsim_specific_train': 0.5331565622312834, 'topsim_test': 0.4911456415751424, 'topsim_specific_test': 0.5337896806994303}\n",
      "dataset (5,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.40910114809391585, 'topsim_specific_train': 0.45750962896306074, 'topsim_test': 0.40793068746116173, 'topsim_specific_test': 0.45940459131649797}\n",
      "dataset (5,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4556657877907706, 'topsim_specific_train': 0.5087685639866619, 'topsim_test': 0.4642131662058412, 'topsim_specific_test': 0.5024660043433963}\n",
      "dataset (5,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4502852596562967, 'topsim_specific_train': 0.5004682794836017, 'topsim_test': 0.4492825315324074, 'topsim_specific_test': 0.5051358955505489}\n"
     ]
    }
   ],
   "source": [
    "# topsim\n",
    "# although topsim values are stored throughout training if callbacks are verbose, we reevaluate the\n",
    "# final topsim scores with more data points \n",
    "\n",
    "samples = 5000\n",
    "for d, dataset in enumerate(datasets):\n",
    "    \n",
    "    dim = [n_values[d]]*n_attributes[d]\n",
    "    \n",
    "    for run in range(5):\n",
    "        print(\"dataset\", dataset, \"run\", run)\n",
    "        \n",
    "        topsim_final = {}\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        #path_to_interaction = (path_to_run + 'interactions/train/interactions_epoch' + str(n_epochs))\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_test = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        \n",
    "        TOPSIM = TopographicSimilarityConceptLevel(dim, is_gumbel=True)\n",
    "        \n",
    "        for mode in ['train', 'test']:\n",
    "\n",
    "            if mode == 'train':\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "            elif mode == 'test':\n",
    "                interaction = torch.load(path_to_interaction_test)\n",
    "                \n",
    "                  \n",
    "            messages = interaction.message.argmax(dim=-1)\n",
    "            sender_input = interaction.sender_input\n",
    "            n_targets = int(sender_input.shape[1]/2)\n",
    "            # get target objects and fixed vectors to re-construct concepts\n",
    "            target_objects = sender_input[:, :n_targets]\n",
    "            target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "            # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "            (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "            # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "            objects = objects + 1\n",
    "            concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "            specific_idx = np.where(np.sum(fixed, axis=1)==n_attributes[d])[0]\n",
    "            #print(specific_idx)\n",
    "            #messages_max_relevant = messages[max_relevant]\n",
    "            #sender_input_max_relevant = sender_input[max_relevant]\n",
    "            messages_specific = messages[specific_idx]\n",
    "            concepts_specific = concepts[specific_idx]\n",
    "            #print(messages_specific[:3])\n",
    "            #print(concepts_specific[:3])\n",
    "\n",
    "            messages = [msg.tolist() for msg in messages]\n",
    "            messages_specific = [msg.tolist() for msg in messages_specific]\n",
    "\n",
    "            #encoded_input = encode_input_for_topsim_hierarchical(sender_input, dim)\n",
    "            #encoded_input_max_relevant = encode_input_for_topsim_hierarchical(sender_input_max_relevant, dim)\n",
    "            encoded_input = encode_target_concepts_for_topsim(sender_input)\n",
    "            topsim = TOPSIM.compute_topsim(encoded_input[0:samples], messages[0:samples])\n",
    "            #topsim_max_relevant = TOPSIM.compute_topsim(encoded_input_max_relevant[0:samples], \n",
    "            #                                            messages_max_relevant[0:samples])\n",
    "            topsim_specific = TOPSIM.compute_topsim(concepts_specific[0:samples], messages_specific[0:samples], \n",
    "                                                    meaning_distance_fn=\"edit\")\n",
    "            print('... topsim computed')\n",
    "\n",
    "            topsim_final['topsim_' + mode] = topsim\n",
    "            topsim_final['topsim_specific_' + mode] = topsim_specific\n",
    "    \n",
    "        pickle.dump(topsim_final, open(path_to_run +  \"topsim_final.pkl\", \"wb\" ) )\n",
    "        print(topsim_final)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set [4, 4, 4]\n",
      "{'posdis_specific': 0.0812758207321167, 'bosdis_specific': 0.21682985126972198, 'posdis': 0.05927928164601326, 'bosdis': 0.22567006945610046}\n",
      "{'posdis_specific': 0.03443637862801552, 'bosdis_specific': 0.22426798939704895, 'posdis': 0.014325053431093693, 'bosdis': 0.20297060906887054}\n",
      "{'posdis_specific': 0.17433571815490723, 'bosdis_specific': 0.33425045013427734, 'posdis': 0.1578224152326584, 'bosdis': 0.3052409291267395}\n",
      "{'posdis_specific': 0.07186233997344971, 'bosdis_specific': 0.20578840374946594, 'posdis': 0.04527847096323967, 'bosdis': 0.2170572280883789}\n",
      "{'posdis_specific': 0.10273190587759018, 'bosdis_specific': 0.2828274667263031, 'posdis': 0.07438632100820541, 'bosdis': 0.2684449851512909}\n",
      "data set [8, 8, 8]\n",
      "{'posdis_specific': 0.11353063583374023, 'bosdis_specific': 0.24216723442077637, 'posdis': 0.10144481062889099, 'bosdis': 0.24038828909397125}\n",
      "{'posdis_specific': 0.2311098426580429, 'bosdis_specific': 0.29010918736457825, 'posdis': 0.19693471491336823, 'bosdis': 0.27640751004219055}\n",
      "{'posdis_specific': 0.20361478626728058, 'bosdis_specific': 0.2746002972126007, 'posdis': 0.18238405883312225, 'bosdis': 0.27631935477256775}\n",
      "{'posdis_specific': 0.08335194736719131, 'bosdis_specific': 0.4049811065196991, 'posdis': 0.06700455397367477, 'bosdis': 0.36812782287597656}\n",
      "{'posdis_specific': 0.12435919046401978, 'bosdis_specific': 0.22735945880413055, 'posdis': 0.1083037257194519, 'bosdis': 0.23266535997390747}\n",
      "data set [16, 16, 16]\n",
      "{'posdis_specific': 0.2752988040447235, 'bosdis_specific': 0.511137843132019, 'posdis': 0.2401740550994873, 'bosdis': 0.49008893966674805}\n",
      "{'posdis_specific': 0.039848823100328445, 'bosdis_specific': 0.1346251219511032, 'posdis': 0.03976787254214287, 'bosdis': 0.1390436738729477}\n",
      "{'posdis_specific': 0.08050676435232162, 'bosdis_specific': 0.15062759816646576, 'posdis': 0.07238855212926865, 'bosdis': 0.16464297473430634}\n",
      "{'posdis_specific': 0.0977076068520546, 'bosdis_specific': 0.17526817321777344, 'posdis': 0.09005415439605713, 'bosdis': 0.17633122205734253}\n",
      "{'posdis_specific': 0.1444317102432251, 'bosdis_specific': 0.169294074177742, 'posdis': 0.13678966462612152, 'bosdis': 0.18183521926403046}\n",
      "data set [4, 4, 4, 4]\n",
      "{'posdis_specific': 0.13709720969200134, 'bosdis_specific': 0.706833004951477, 'posdis': 0.10238486528396606, 'bosdis': 0.5076914429664612}\n",
      "{'posdis_specific': 0.2237592190504074, 'bosdis_specific': 0.7067229747772217, 'posdis': 0.13931119441986084, 'bosdis': 0.4871298372745514}\n",
      "{'posdis_specific': 0.07304670661687851, 'bosdis_specific': 0.8779588937759399, 'posdis': 0.049992647022008896, 'bosdis': 0.5820725560188293}\n",
      "{'posdis_specific': 0.070336252450943, 'bosdis_specific': 0.4540296494960785, 'posdis': 0.07257960736751556, 'bosdis': 0.385468065738678}\n",
      "{'posdis_specific': 0.34370601177215576, 'bosdis_specific': 0.639029324054718, 'posdis': 0.18659411370754242, 'bosdis': 0.46163901686668396}\n",
      "data set [8, 8, 8, 8]\n",
      "{'posdis_specific': 0.15182185173034668, 'bosdis_specific': 0.3634018003940582, 'posdis': 0.12788040935993195, 'bosdis': 0.3589456081390381}\n",
      "{'posdis_specific': 0.1213153675198555, 'bosdis_specific': 0.24209724366664886, 'posdis': 0.10665355622768402, 'bosdis': 0.2361060082912445}\n",
      "{'posdis_specific': 0.07844514399766922, 'bosdis_specific': 0.5524210333824158, 'posdis': 0.07449708133935928, 'bosdis': 0.48816293478012085}\n",
      "{'posdis_specific': 0.05806409567594528, 'bosdis_specific': 0.6404103636741638, 'posdis': 0.029537610709667206, 'bosdis': 0.586575448513031}\n",
      "{'posdis_specific': 0.07757384330034256, 'bosdis_specific': 0.235956072807312, 'posdis': 0.0668351948261261, 'bosdis': 0.22515858709812164}\n",
      "data set [4, 4, 4, 4, 4]\n",
      "{'posdis_specific': 0.10120495408773422, 'bosdis_specific': 0.47249919176101685, 'posdis': 0.06760774552822113, 'bosdis': 0.38182246685028076}\n",
      "{'posdis_specific': 0.12179650366306305, 'bosdis_specific': 0.40446698665618896, 'posdis': 0.06720991432666779, 'bosdis': 0.3190315365791321}\n",
      "{'posdis_specific': 0.07032281905412674, 'bosdis_specific': 0.23899006843566895, 'posdis': 0.05760541558265686, 'bosdis': 0.16431361436843872}\n",
      "{'posdis_specific': 0.16635951399803162, 'bosdis_specific': 0.17821311950683594, 'posdis': 0.11498210579156876, 'bosdis': 0.16067634522914886}\n",
      "{'posdis_specific': 0.12630972266197205, 'bosdis_specific': 0.348008930683136, 'posdis': 0.07398245483636856, 'bosdis': 0.29100915789604187}\n"
     ]
    }
   ],
   "source": [
    "# use Disent callback from egg\n",
    "\n",
    "for d in range(len(datasets)): \n",
    "    \n",
    "    path = paths[d]\n",
    "    dim = [n_values[d]] * n_attributes[d]\n",
    "    n_features = n_attributes[d] * n_values[d]\n",
    "    vs_factor = int(path[-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    print(\"data set\", dim)\n",
    "    \n",
    "    for run in range(5):\n",
    "        \n",
    "        posdis_bosdis = {}\n",
    "    \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        \n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sender_input = interaction.sender_input\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "        # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "        objects = objects + 1\n",
    "        concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "\n",
    "        # concrete/specific concepts: where all attributes are fixed\n",
    "        #objects_max_relevance = torch.tensor(k_hot_to_attributes(\n",
    "        #    objects[torch.sum(relevance_vectors, dim=1) == 0], dim[0]))\n",
    "        #messages_max_relevance = messages[torch.sum(relevance_vectors, dim=1) == 0]\n",
    "        #concepts_specific = torch.tensor(k_hot_to_attributes(\n",
    "        #    objects[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes], dim[0]))\n",
    "        concepts_specific = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]])\n",
    "        messages_specific = messages[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]]\n",
    "        \n",
    "        #posdis_max_relevance = Disent.posdis(objects_max_relevance, messages_max_relevance)\n",
    "        #bosdis_max_relevance = Disent.bosdis(objects_max_relevance, messages_max_relevance, vocab_size)\n",
    "        posdis_specific = Disent.posdis(concepts_specific, messages_specific)\n",
    "        bosdis_specific = Disent.bosdis(concepts_specific, messages_specific, vocab_size)\n",
    "        \n",
    "        #targets_encoded = torch.tensor(encode_target_concepts_for_topsim(sender_input))\n",
    "        #objects = torch.tensor(k_hot_to_attributes(targets_encoded, n_values[d]+1))\n",
    "        posdis = Disent.posdis(torch.from_numpy(objects), messages)\n",
    "        bosdis = Disent.bosdis(torch.from_numpy(objects), messages, vocab_size)\n",
    "        \n",
    "        posdis_bosdis['posdis_specific'] = posdis_specific\n",
    "        posdis_bosdis['bosdis_specific'] = bosdis_specific\n",
    "        posdis_bosdis['posdis'] = posdis\n",
    "        posdis_bosdis['bosdis'] = bosdis\n",
    "\n",
    "        print(posdis_bosdis)\n",
    "    \n",
    "        pickle.dump(posdis_bosdis, open(path_to_run + \"posdis_bosdis.pkl\", \"wb\" ) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet implemented:\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    vs_factor = int(paths[d][-2])\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        \n",
    "        scores = cooccurrence_per_hierarchy_level(interaction, attributes, values, vs_factor)\n",
    "\n",
    "        print(scores)\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'normalized_cooccurrence.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
