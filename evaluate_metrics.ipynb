{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils.analysis_from_interaction import *\n",
    "from egg.core.language_analysis import Disent\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate metrics from stored interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ('(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)')\n",
    "n_attributes = (3, 3, 3, 4, 4, 5)\n",
    "n_values = (4, 8, 16, 4, 8, 4)\n",
    "n_epochs = 300\n",
    "#paths = ['results/' + d + '_sample_scaling_10_balanced_False_vsf_3/' for d in datasets]\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "length_cost = False\n",
    "if context_unaware:\n",
    "    setting = 'context_unaware'\n",
    "elif length_cost:\n",
    "    setting = 'length_cost'\n",
    "else:\n",
    "    setting = 'standard'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy scores: MI, effectiveness, efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: get interactions on local Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/' \n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = information_scores(interaction, attributes, values, normalizer=\"arithmetic\")\n",
    "\n",
    "        pickle.dump(scores, open(path_to_run + 'entropy_scores.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we evaluated message length per hierarchy level after training but \n",
    "# you can also use the HierarchicalMessageLength callback and store the results \n",
    "# TODO: Message length results look weird, needs to be fixed!\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        #path_to_interaction = (path_to_run + 'interactions/train/interactions_epoch' + str(n_epochs))\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = message_length_per_hierarchy_level(interaction, attributes)\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'message_length_hierarchical.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  symbol redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    attributes = n_attributes[d]\n",
    "    values = n_values[d]\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        #symbol_f = np.load(path_to_run + 'symbols_pernsame.npy')\n",
    "        #path_to_interaction = (path_to_run + 'interactions/train/interactions_epoch' + str(n_epochs))\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "        redundancy, MI = symbol_frequency(interaction, attributes, values, vocab_size)\n",
    "        \n",
    "        scores = {'symbol_redundancy': redundancy, 'MI_symbol-attribute_value': MI}\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'symbol_redundancy.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  compositionality scores: topsim, posdis, bosdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset (3,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.38871896826133084, 'topsim_specific_train': 0.43321318492691, 'topsim_generic_train': 0.36897570310869693, 'topsim_val': 0.38018437819239875, 'topsim_specific_val': 0.42933033125052, 'topsim_generic_val': 0.3245027995418074}\n",
      "dataset (3,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.34181238371209255, 'topsim_specific_train': 0.4019049024609206, 'topsim_generic_train': 0.2537582454872124, 'topsim_val': 0.32402785884203955, 'topsim_specific_val': 0.38064854233526485, 'topsim_generic_val': 0.27693569221576597}\n",
      "dataset (3,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4742645917733958, 'topsim_specific_train': 0.5304615521685855, 'topsim_generic_train': 0.5864191549848387, 'topsim_val': 0.5056998361966318, 'topsim_specific_val': 0.5401416198156687, 'topsim_generic_val': 0.3237550653401823}\n",
      "dataset (3,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.39053030315528126, 'topsim_specific_train': 0.47081705350518205, 'topsim_generic_train': 0.2390048029035228, 'topsim_val': 0.38284044125840955, 'topsim_specific_val': 0.47517813303574413, 'topsim_generic_val': 0.21661224955643837}\n",
      "dataset (3,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.33781815264592896, 'topsim_specific_train': 0.38611553831015194, 'topsim_generic_train': 0.2928134059888926, 'topsim_val': 0.36980150800031336, 'topsim_specific_val': 0.407184035857432, 'topsim_generic_val': 0.4370149180386296}\n",
      "dataset (3,8) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.2364136681412218, 'topsim_specific_train': 0.2825110008932517, 'topsim_generic_train': 0.3432035675339836, 'topsim_val': 0.24714698450425565, 'topsim_specific_val': 0.29316773556615205, 'topsim_generic_val': 0.3541746905838002}\n",
      "dataset (3,8) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.37160833698187457, 'topsim_specific_train': 0.4491368763315504, 'topsim_generic_train': 0.4075060669334944, 'topsim_val': 0.370608987799683, 'topsim_specific_val': 0.4549430160808681, 'topsim_generic_val': 0.2718107913106112}\n",
      "dataset (3,8) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.2402839464970421, 'topsim_specific_train': 0.27587100397149417, 'topsim_generic_train': 0.4127011014895831, 'topsim_val': 0.24049291065672232, 'topsim_specific_val': 0.2742962697590571, 'topsim_generic_val': 0.31801729236581683}\n",
      "dataset (3,8) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.24403548217605428, 'topsim_specific_train': 0.2810061875368435, 'topsim_generic_train': 0.36334097784379504, 'topsim_val': 0.2392929322254886, 'topsim_specific_val': 0.27701561352754434, 'topsim_generic_val': 0.20351941094544665}\n",
      "dataset (3,8) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.26230245163460747, 'topsim_specific_train': 0.2985789236348016, 'topsim_generic_train': 0.2533456855031074, 'topsim_val': 0.2688829945296385, 'topsim_specific_val': 0.3019889882447595, 'topsim_generic_val': 0.3296719045230275}\n",
      "dataset (3,16) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.19856756057499242, 'topsim_specific_train': 0.2402447332602743, 'topsim_generic_train': 0.35799965244081444, 'topsim_val': 0.20162223170507482, 'topsim_specific_val': 0.24075310330248145, 'topsim_generic_val': 0.29989715097420433}\n",
      "dataset (3,16) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.17110635486313452, 'topsim_specific_train': 0.19989782070577738, 'topsim_generic_train': 0.42690379569137515, 'topsim_val': 0.16882015320523477, 'topsim_specific_val': 0.19732933321316942, 'topsim_generic_val': 0.3835526928969255}\n",
      "dataset (3,16) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.16996775059934255, 'topsim_specific_train': 0.21065026041928042, 'topsim_generic_train': 0.4557709622842579, 'topsim_val': 0.1689826736930857, 'topsim_specific_val': 0.20768987095503624, 'topsim_generic_val': 0.35119502265049635}\n",
      "dataset (3,16) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.1817600347841686, 'topsim_specific_train': 0.21569465162295795, 'topsim_generic_train': 0.300831052886682, 'topsim_val': 0.18882325862257965, 'topsim_specific_val': 0.22144508136946145, 'topsim_generic_val': 0.21815597860827887}\n",
      "dataset (3,16) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.20432743804286496, 'topsim_specific_train': 0.2504833408119631, 'topsim_generic_train': 0.2185846485016238, 'topsim_val': 0.21004287208703482, 'topsim_specific_val': 0.2510376526563107, 'topsim_generic_val': 0.21683437412110781}\n",
      "dataset (4,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3958848589266688, 'topsim_specific_train': 0.4699011430123021, 'topsim_generic_train': 0.3144866153118582, 'topsim_val': 0.39635095180561675, 'topsim_specific_val': 0.46793495262705675, 'topsim_generic_val': 0.33600440825980443}\n",
      "dataset (4,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.44965089671068775, 'topsim_specific_train': 0.5139691999770046, 'topsim_generic_train': 0.26581172996946567, 'topsim_val': 0.4415000001049762, 'topsim_specific_val': 0.5090652738649544, 'topsim_generic_val': 0.24023211389699536}\n",
      "dataset (4,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.398912785460775, 'topsim_specific_train': 0.4737587037567698, 'topsim_generic_train': 0.22496435295957706, 'topsim_val': 0.40275883200983925, 'topsim_specific_val': 0.4781493800317069, 'topsim_generic_val': 0.21331488569101983}\n",
      "dataset (4,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4573774431877239, 'topsim_specific_train': 0.5214910676545507, 'topsim_generic_train': 0.33998496518824667, 'topsim_val': 0.4634706066960188, 'topsim_specific_val': 0.5187689419787608, 'topsim_generic_val': 0.16220299655934659}\n",
      "dataset (4,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3841278616332016, 'topsim_specific_train': 0.4485768869144447, 'topsim_generic_train': 0.443468004477662, 'topsim_val': 0.3907509010411172, 'topsim_specific_val': 0.4538004661892032, 'topsim_generic_val': 0.4002727070890288}\n",
      "dataset (4,8) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.33902507899489315, 'topsim_specific_train': 0.3911358497359046, 'topsim_generic_train': 0.4090450154195942, 'topsim_val': 0.3275442160714104, 'topsim_specific_val': 0.3738696711921182, 'topsim_generic_val': 0.4314435187804927}\n",
      "dataset (4,8) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4337077335542268, 'topsim_specific_train': 0.5140183964367973, 'topsim_generic_train': 0.5803588343058985, 'topsim_val': 0.4161598366328328, 'topsim_specific_val': 0.48905251906892366, 'topsim_generic_val': 0.6050576004908925}\n",
      "dataset (4,8) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.28662740427874484, 'topsim_specific_train': 0.32280102438881003, 'topsim_generic_train': 0.39441503160112246, 'topsim_val': 0.2840022642136215, 'topsim_specific_val': 0.3238647699080334, 'topsim_generic_val': 0.35778117189315856}\n",
      "dataset (4,8) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.40728652092443646, 'topsim_specific_train': 0.4891782550705217, 'topsim_generic_train': 0.5780778264453112, 'topsim_val': 0.4143299275554465, 'topsim_specific_val': 0.49395674253745814, 'topsim_generic_val': 0.5033050298866549}\n",
      "dataset (4,8) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3288985897571663, 'topsim_specific_train': 0.38961579664791685, 'topsim_generic_train': 0.47751313037924514, 'topsim_val': 0.34270482897658866, 'topsim_specific_val': 0.3973646160594084, 'topsim_generic_val': 0.4750901433959788}\n",
      "dataset (5,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.3531869088155256, 'topsim_specific_train': 0.37676156343090517, 'topsim_generic_train': 0.41628312787272725, 'topsim_val': 0.35494491674087625, 'topsim_specific_val': 0.3767877477024841, 'topsim_generic_val': 0.3708181517025867}\n",
      "dataset (5,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.42529944953753895, 'topsim_specific_train': 0.4461802748574615, 'topsim_generic_train': 0.19155307812991726, 'topsim_val': 0.4312799300619956, 'topsim_specific_val': 0.46284784025935566, 'topsim_generic_val': 0.20030998196572358}\n",
      "dataset (5,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.4657183694822873, 'topsim_specific_train': 0.5337533947596351, 'topsim_generic_train': 0.4347362555158756, 'topsim_val': 0.4651885278164177, 'topsim_specific_val': 0.5331397223279614, 'topsim_generic_val': 0.4637664148176378}\n",
      "dataset (5,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.45875730919267377, 'topsim_specific_train': 0.4927793088008645, 'topsim_generic_train': 0.26395309301812303, 'topsim_val': 0.48009079445558317, 'topsim_specific_val': 0.5167954546390036, 'topsim_generic_val': 0.21016483967227323}\n",
      "dataset (5,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.35225750689284274, 'topsim_specific_train': 0.38561656332678573, 'topsim_generic_train': 0.17612779342039875, 'topsim_val': 0.35594928462743664, 'topsim_specific_val': 0.3926624842441642, 'topsim_generic_val': 0.18392748436273917}\n"
     ]
    }
   ],
   "source": [
    "# topsim\n",
    "# although topsim values are stored throughout training if callbacks are verbose, we reevaluate the\n",
    "# final topsim scores with more data points \n",
    "\n",
    "samples = 5000 # maybe shuffle from these because otherwise I just take the first 5,000 (which might not be the best)\n",
    "for d, dataset in enumerate(datasets):\n",
    "    \n",
    "    dim = [n_values[d]]*n_attributes[d]\n",
    "    \n",
    "    for run in range(5):\n",
    "        print(\"dataset\", dataset, \"run\", run)\n",
    "        \n",
    "        topsim_final = {}\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        \n",
    "        TOPSIM = TopographicSimilarityConceptLevel(dim, is_gumbel=True)\n",
    "        \n",
    "        for mode in ['train', 'val']:\n",
    "\n",
    "            if mode == 'train':\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "            elif mode == 'val':\n",
    "                interaction = torch.load(path_to_interaction_val)\n",
    "                \n",
    "                  \n",
    "            messages = interaction.message.argmax(dim=-1)\n",
    "            sender_input = interaction.sender_input\n",
    "            n_targets = int(sender_input.shape[1]/2)\n",
    "            # get target objects and fixed vectors to re-construct concepts\n",
    "            target_objects = sender_input[:, :n_targets]\n",
    "            target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "            # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "            (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "            # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "            objects = objects + 1\n",
    "            concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "            specific_idx = np.where(np.sum(fixed, axis=1)==n_attributes[d])[0]\n",
    "            messages_specific = messages[specific_idx]\n",
    "            concepts_specific = concepts[specific_idx]\n",
    "            \n",
    "            generic_idx = np.where(np.sum(fixed, axis=1)==1)[0]\n",
    "            messages_generic = messages[generic_idx]\n",
    "            concepts_generic = concepts[generic_idx]\n",
    "\n",
    "            messages = [msg.tolist() for msg in messages]\n",
    "            messages_specific = [msg.tolist() for msg in messages_specific]\n",
    "            messages_generic = [msg.tolist() for msg in messages_generic]\n",
    "\n",
    "            encoded_input = encode_target_concepts_for_topsim(sender_input)\n",
    "            # randomly take samples when more than 5000 samples are available\n",
    "            # if len(encoded_input) > samples: \n",
    "            #     print(\"sampling\")\n",
    "            #     sample_indices = random.sample(range(len(encoded_input)), samples)\n",
    "            #     sampled_input = [encoded_input[i] for i in sample_indices]\n",
    "            #     sampled_messages = [messages[i] for i in sample_indices]\n",
    "            #     print(\"start computing\")\n",
    "            #     print(len(sampled_input), len(sampled_input[0]), len(sampled_input[0][0]))\n",
    "            #     topsim = TOPSIM.compute_topsim(sampled_input, sampled_messages)\n",
    "            # else:\n",
    "            topsim = TOPSIM.compute_topsim(encoded_input[0:samples], messages[0:samples]) # default: hausdorff distance for concepts, edit distance for messages\n",
    "            # if len(concepts_specific) > samples:\n",
    "            #     print(\"sampling specific\")\n",
    "            #     sample_indices_specific = random.sample(range(len(concepts_specific)), samples)\n",
    "            #     sampled_input_specific = [concepts_specific[i] for i in sample_indices_specific]\n",
    "            #     sampled_messages_specific = [messages_specific[i] for i in sample_indices_specific]\n",
    "            #     topsim_specific = TOPSIM.compute_topsim(sampled_input_specific, sampled_messages_specific, \n",
    "            #                                             meaning_distance_fn=\"edit\")\n",
    "            # else:\n",
    "            topsim_specific = TOPSIM.compute_topsim(concepts_specific[0:samples], messages_specific[0:samples], \n",
    "                                                        meaning_distance_fn=\"edit\")\n",
    "            \n",
    "            topsim_generic = TOPSIM.compute_topsim(concepts_generic[0:samples], messages_generic[0:samples],\n",
    "                                                   meaning_distance_fn=\"edit\")\n",
    "\n",
    "            print('... topsim computed')\n",
    "\n",
    "            topsim_final['topsim_' + mode] = topsim\n",
    "            topsim_final['topsim_specific_' + mode] = topsim_specific\n",
    "            topsim_final['topsim_generic_' + mode] = topsim_generic\n",
    "    \n",
    "        pickle.dump(topsim_final, open(path_to_run +  \"topsim_final.pkl\", \"wb\" ) )\n",
    "        print(topsim_final)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set [4, 4, 4]\n",
      "{'posdis_specific': 0.1343628317117691, 'bosdis_specific': 0.19703423976898193, 'posdis_generic': 0.012363173998892307, 'bosdis_generic': 0.25586870312690735, 'posdis': 0.11593814939260483, 'bosdis': 0.20009183883666992}\n",
      "{'posdis_specific': 0.07404900342226028, 'bosdis_specific': 0.11321671307086945, 'posdis_generic': 0.05984930321574211, 'bosdis_generic': 0.20419056713581085, 'posdis': 0.05361713469028473, 'bosdis': 0.10996849834918976}\n",
      "{'posdis_specific': 0.21100080013275146, 'bosdis_specific': 0.2960694432258606, 'posdis_generic': 0.025247842073440552, 'bosdis_generic': 0.22759351134300232, 'posdis': 0.1737801879644394, 'bosdis': 0.2586870491504669}\n",
      "{'posdis_specific': 0.0761677548289299, 'bosdis_specific': 0.30827760696411133, 'posdis_generic': 0.051543522626161575, 'bosdis_generic': 0.2636752426624298, 'posdis': 0.053765375167131424, 'bosdis': 0.2669734060764313}\n",
      "{'posdis_specific': 0.11557599902153015, 'bosdis_specific': 0.14247490465641022, 'posdis_generic': 0.061901938170194626, 'bosdis_generic': 0.20893122255802155, 'posdis': 0.09373592585325241, 'bosdis': 0.1343991607427597}\n",
      "data set [8, 8, 8]\n",
      "{'posdis_specific': 0.09279877692461014, 'bosdis_specific': 0.13518096506595612, 'posdis_generic': 0.0233264472335577, 'bosdis_generic': 0.23110292851924896, 'posdis': 0.08148249238729477, 'bosdis': 0.13984084129333496}\n",
      "{'posdis_specific': 0.22045283019542694, 'bosdis_specific': 0.23112332820892334, 'posdis_generic': 0.04733671620488167, 'bosdis_generic': 0.2668977975845337, 'posdis': 0.1931949257850647, 'bosdis': 0.2319338172674179}\n",
      "{'posdis_specific': 0.007731779012829065, 'bosdis_specific': 0.16391442716121674, 'posdis_generic': 0.01598147116601467, 'bosdis_generic': 0.21031329035758972, 'posdis': 0.008150085806846619, 'bosdis': 0.1720353364944458}\n",
      "{'posdis_specific': 0.10669281333684921, 'bosdis_specific': 0.1263131946325302, 'posdis_generic': 0.02449287474155426, 'bosdis_generic': 0.19570663571357727, 'posdis': 0.1011422649025917, 'bosdis': 0.13230039179325104}\n",
      "{'posdis_specific': 0.024739429354667664, 'bosdis_specific': 0.19297237694263458, 'posdis_generic': 0.06422652304172516, 'bosdis_generic': 0.17382390797138214, 'posdis': 0.034940365701913834, 'bosdis': 0.18921414017677307}\n",
      "data set [16, 16, 16]\n",
      "{'posdis_specific': 0.14062905311584473, 'bosdis_specific': 0.18828913569450378, 'posdis_generic': 0.07644618302583694, 'bosdis_generic': 0.15938682854175568, 'posdis': 0.13635863363742828, 'bosdis': 0.19069193303585052}\n",
      "{'posdis_specific': 0.119649738073349, 'bosdis_specific': 0.12361442297697067, 'posdis_generic': 0.03753972053527832, 'bosdis_generic': 0.10644129663705826, 'posdis': 0.10953810065984726, 'bosdis': 0.12036745995283127}\n",
      "{'posdis_specific': 0.0890875980257988, 'bosdis_specific': 0.12769842147827148, 'posdis_generic': 0.030200457200407982, 'bosdis_generic': 0.1288745105266571, 'posdis': 0.09110236912965775, 'bosdis': 0.13305971026420593}\n",
      "{'posdis_specific': 0.07652799040079117, 'bosdis_specific': 0.1404700130224228, 'posdis_generic': 0.05152000114321709, 'bosdis_generic': 0.15464791655540466, 'posdis': 0.0732097402215004, 'bosdis': 0.15105946362018585}\n",
      "{'posdis_specific': 0.02633158676326275, 'bosdis_specific': 0.25188136100769043, 'posdis_generic': 0.05463181063532829, 'bosdis_generic': 0.20367030799388885, 'posdis': 0.01876164600253105, 'bosdis': 0.25199633836746216}\n",
      "data set [4, 4, 4, 4]\n",
      "{'posdis_specific': 0.19534622132778168, 'bosdis_specific': 0.19174665212631226, 'posdis_generic': 0.03259387984871864, 'bosdis_generic': 0.1560015082359314, 'posdis': 0.13083937764167786, 'bosdis': 0.17409588396549225}\n",
      "{'posdis_specific': 0.2148803174495697, 'bosdis_specific': 0.2850704789161682, 'posdis_generic': 0.06422944366931915, 'bosdis_generic': 0.1477663815021515, 'posdis': 0.1676202416419983, 'bosdis': 0.25716838240623474}\n",
      "{'posdis_specific': 0.20744262635707855, 'bosdis_specific': 0.24644474685192108, 'posdis_generic': 0.0470484159886837, 'bosdis_generic': 0.05057764798402786, 'posdis': 0.14393824338912964, 'bosdis': 0.19187544286251068}\n",
      "{'posdis_specific': 0.13798730075359344, 'bosdis_specific': 0.34820735454559326, 'posdis_generic': 0.018193379044532776, 'bosdis_generic': 0.19167089462280273, 'posdis': 0.11374420672655106, 'bosdis': 0.31380900740623474}\n",
      "{'posdis_specific': 0.13839338719844818, 'bosdis_specific': 0.19009371101856232, 'posdis_generic': 0.05984713137149811, 'bosdis_generic': 0.13086651265621185, 'posdis': 0.11434854567050934, 'bosdis': 0.1728871464729309}\n",
      "data set [8, 8, 8, 8]\n",
      "{'posdis_specific': 0.2527528405189514, 'bosdis_specific': 0.21184402704238892, 'posdis_generic': 0.10807424038648605, 'bosdis_generic': 0.19836905598640442, 'posdis': 0.2193526029586792, 'bosdis': 0.2211270034313202}\n",
      "{'posdis_specific': 0.42977166175842285, 'bosdis_specific': 0.5126911997795105, 'posdis_generic': 0.22087007761001587, 'bosdis_generic': 0.20510774850845337, 'posdis': 0.39122724533081055, 'bosdis': 0.4641494154930115}\n",
      "{'posdis_specific': 0.028297416865825653, 'bosdis_specific': 0.2690012753009796, 'posdis_generic': 0.05948144569993019, 'bosdis_generic': 0.17571595311164856, 'posdis': 0.020917396992444992, 'bosdis': 0.2549881339073181}\n",
      "{'posdis_specific': 0.45290109515190125, 'bosdis_specific': 0.37748342752456665, 'posdis_generic': 0.2042452096939087, 'bosdis_generic': 0.17194683849811554, 'posdis': 0.39813536405563354, 'bosdis': 0.3364599645137787}\n",
      "{'posdis_specific': 0.1919795125722885, 'bosdis_specific': 0.17193986475467682, 'posdis_generic': 0.09959898889064789, 'bosdis_generic': 0.13451474905014038, 'posdis': 0.18101660907268524, 'bosdis': 0.16243813931941986}\n",
      "data set [4, 4, 4, 4, 4]\n",
      "{'posdis_specific': 0.15073375403881073, 'bosdis_specific': 0.26227086782455444, 'posdis_generic': 0.047555938363075256, 'bosdis_generic': 0.13416196405887604, 'posdis': 0.11570727825164795, 'bosdis': 0.22857719659805298}\n",
      "{'posdis_specific': 0.1165398508310318, 'bosdis_specific': 0.2961806654930115, 'posdis_generic': 0.025581782683730125, 'bosdis_generic': 0.10776078701019287, 'posdis': 0.08959878981113434, 'bosdis': 0.2537742257118225}\n",
      "{'posdis_specific': 0.16372275352478027, 'bosdis_specific': 0.32045140862464905, 'posdis_generic': 0.12697434425354004, 'bosdis_generic': 0.11992564052343369, 'posdis': 0.13416685163974762, 'bosdis': 0.24849708378314972}\n",
      "{'posdis_specific': 0.15557760000228882, 'bosdis_specific': 0.38914865255355835, 'posdis_generic': 0.05059418827295303, 'bosdis_generic': 0.1372111588716507, 'posdis': 0.13295838236808777, 'bosdis': 0.3281954526901245}\n",
      "{'posdis_specific': 0.15531983971595764, 'bosdis_specific': 0.15066498517990112, 'posdis_generic': 0.021811744198203087, 'bosdis_generic': 0.08975869417190552, 'posdis': 0.109475277364254, 'bosdis': 0.14351895451545715}\n"
     ]
    }
   ],
   "source": [
    "# use Disent callback from egg\n",
    "\n",
    "for d in range(len(datasets)): \n",
    "    \n",
    "    path = paths[d]\n",
    "    dim = [n_values[d]] * n_attributes[d]\n",
    "    n_features = n_attributes[d] * n_values[d]\n",
    "    vs_factor = int(path[-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    print(\"data set\", dim)\n",
    "    \n",
    "    for run in range(5):\n",
    "        \n",
    "        posdis_bosdis = {}\n",
    "    \n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        \n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sender_input = interaction.sender_input\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "        # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "        objects = objects + 1\n",
    "        concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "\n",
    "        # concrete/specific concepts: where all attributes are fixed\n",
    "        concepts_specific = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]])\n",
    "        messages_specific = messages[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]]\n",
    "\n",
    "        # generic concepts: where only one attribute is fixed\n",
    "        concepts_generic = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == 1])\n",
    "        messages_generic = messages[torch.sum(torch.from_numpy(fixed), dim=1) == 1]\n",
    "        \n",
    "        posdis_specific = Disent.posdis(concepts_specific, messages_specific)\n",
    "        bosdis_specific = Disent.bosdis(concepts_specific, messages_specific, vocab_size)\n",
    "\n",
    "        posdis_generic = Disent.posdis(concepts_generic, messages_generic)\n",
    "        bosdis_generic = Disent.bosdis(concepts_generic, messages_generic, vocab_size)\n",
    "        \n",
    "        posdis = Disent.posdis(torch.from_numpy(objects), messages)\n",
    "        bosdis = Disent.bosdis(torch.from_numpy(objects), messages, vocab_size)\n",
    "        \n",
    "        posdis_bosdis['posdis_specific'] = posdis_specific\n",
    "        posdis_bosdis['bosdis_specific'] = bosdis_specific\n",
    "        posdis_bosdis['posdis_generic'] = posdis_generic\n",
    "        posdis_bosdis['bosdis_generic'] = bosdis_generic\n",
    "        posdis_bosdis['posdis'] = posdis\n",
    "        posdis_bosdis['bosdis'] = bosdis\n",
    "\n",
    "        print(posdis_bosdis)\n",
    "    \n",
    "        pickle.dump(posdis_bosdis, open(path_to_run + \"posdis_bosdis.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(datasets)):\n",
    "\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/' \n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = bosdis(interaction, attributes, values, vocab_size)\n",
    "\n",
    "        pickle.dump(scores, open(path_to_run + 'bosdis_scores.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet implemented:\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    vs_factor = int(paths[d][-2])\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        \n",
    "        scores = cooccurrence_per_hierarchy_level(interaction, attributes, values, vs_factor)\n",
    "\n",
    "        print(scores)\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'normalized_cooccurrence.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
